{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features with 'readmitted':\n",
      "readmitted                  1.000000\n",
      "number_inpatient            0.166616\n",
      "number_diagnoses            0.101320\n",
      "number_emergency            0.086318\n",
      "number_outpatient           0.081094\n",
      "diabetesMed                 0.055338\n",
      "admission_source_id         0.040878\n",
      "time_in_hospital            0.037187\n",
      "num_medications             0.034690\n",
      "num_lab_procedures          0.034048\n",
      "age                         0.025599\n",
      "repaglinide                 0.017146\n",
      "acarbose                    0.016817\n",
      "glipizide                   0.016816\n",
      "rosiglitazone               0.015029\n",
      "pioglitazone                0.014171\n",
      "A1Cresult                   0.009288\n",
      "glimepiride                 0.005256\n",
      "diag_3                      0.004434\n",
      "nateglinide                 0.004388\n",
      "glipizide-metformin         0.004310\n",
      "glimepiride-pioglitazone    0.004132\n",
      "acetohexamide               0.004132\n",
      "chlorpropamide              0.003728\n",
      "miglitol                    0.003290\n",
      "glyburide-metformin         0.003257\n",
      "troglitazone                0.003226\n",
      "insulin                     0.001759\n",
      "diag_2                      0.001172\n",
      "admission_type_id          -0.000494\n",
      "tolbutamide                -0.002342\n",
      "metformin-pioglitazone     -0.002677\n",
      "tolazamide                 -0.003727\n",
      "metformin-rosiglitazone    -0.003786\n",
      "glyburide                  -0.004922\n",
      "diag_1                     -0.006575\n",
      "max_glu_serum              -0.010900\n",
      "gender                     -0.018363\n",
      "race                       -0.019303\n",
      "metformin                  -0.020103\n",
      "weight                     -0.036612\n",
      "discharge_disposition_id   -0.037065\n",
      "change                     -0.041717\n",
      "num_procedures             -0.043374\n",
      "examide                          NaN\n",
      "citoglipton                      NaN\n",
      "Name: readmitted, dtype: float64\n",
      "Preprocessed dataset saved to dataset/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'dataset/diabetic_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Replace '?' with NaN\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Step 2: Convert 'readmitted' to binary (0 for NO, 1 for >30 or <=30)\n",
    "data['readmitted'] = data['readmitted'].map({'NO': 0, '>30': 1, '<=30': 1})\n",
    "\n",
    "# Step 3: Drop unnecessary columns\n",
    "columns_to_drop = ['encounter_id', 'patient_nbr', 'payer_code', 'medical_specialty']\n",
    "data.drop(columns=columns_to_drop, inplace=True, axis=1)\n",
    "\n",
    "# Step 4: Separate features into categorical and numeric\n",
    "categorical_cols = data.select_dtypes(include='object').columns\n",
    "numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Step 5: Define preprocessing for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors in a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Step 6: Apply preprocessing\n",
    "processed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "# Extract feature names for one-hot encoded categorical columns\n",
    "categorical_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "all_feature_names = numeric_cols.tolist() + categorical_feature_names.tolist()\n",
    "\n",
    "# Convert processed data to a DataFrame\n",
    "processed_df = pd.DataFrame(processed_data, columns=all_feature_names)\n",
    "\n",
    "# Add the target column back\n",
    "processed_df['readmitted'] = data['readmitted'].values\n",
    "\n",
    "# Step 7: Save processed data to CSV\n",
    "output_file = 'dataset/preprocessed_data.csv'\n",
    "processed_df.to_csv(output_file, index=False)\n",
    "print(f\"Preprocessed dataset saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (81412, 45)\n",
      "Testing features shape: (20354, 45)\n",
      "Training target shape: (81412,)\n",
      "Testing target shape: (20354,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('dataset/preprocessed_data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "target = df['readmitted']\n",
    "features = df.drop('readmitted', axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, \n",
    "                                                    test_size=0.2, random_state=0)\n",
    "\n",
    "# Verify the split\n",
    "print(\"Training features shape:\", x_train.shape)\n",
    "print(\"Testing features shape:\", x_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m best_f1_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pipeline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pipelines):\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(x_test)  \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Calculate precision, recall, f1-score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1000304936\\Documents\\CDS503_ML\\CDS503-ML-Group-Project\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1000304936\\Documents\\CDS503_ML\\CDS503-ML-Group-Project\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:660\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    655\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    656\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    657\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    658\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    659\u001b[0m         )\n\u001b[1;32m--> 660\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\1000304936\\Documents\\CDS503_ML\\CDS503-ML-Group-Project\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1000304936\\Documents\\CDS503_ML\\CDS503-ML-Group-Project\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1019\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1019\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\1000304936\\Documents\\CDS503_ML\\CDS503-ML-Group-Project\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:294\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classification:\n\u001b[1;32m--> 294\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(y)\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\1000304936\\Documents\\CDS503_ML\\CDS503-ML-Group-Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:222\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    214\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m ]:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "# Define pipelines\n",
    "\n",
    "# Decision Tree\n",
    "pl_std_tree = Pipeline([('Standard Scaler', StandardScaler()),\n",
    "                        ('DTClassifier', DecisionTreeClassifier())]) \n",
    "pl_mm_tree = Pipeline([('Min Max Scaler', MinMaxScaler()),\n",
    "                       ('DTClassifier', DecisionTreeClassifier())]) \n",
    "# KNN\n",
    "pl_std_knn = Pipeline([('Standard Scaler', StandardScaler()),\n",
    "                       ('KNClassifier', KNeighborsClassifier())]) \n",
    "pl_mm_knn = Pipeline([('Min Max Scaler', MinMaxScaler()),\n",
    "                      ('KNClassifier', KNeighborsClassifier())])\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "pl_mm_mnb = Pipeline([('Min Max Scaler', MinMaxScaler()),\n",
    "                      ('MNBClassifier', MultinomialNB())])\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "pl_mm_bnb = Pipeline([('Min Max Scaler', MinMaxScaler()),\n",
    "                      ('BNBClassifier', BernoulliNB())])\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "pl_std_gnb = Pipeline([('Standard Scaler', StandardScaler()),\n",
    "                       ('GNBClassifier', GaussianNB())])\n",
    "\n",
    "# SVM\n",
    "pl_std_svc = Pipeline([('Standard Scaler', StandardScaler()),\n",
    "                       ('SVClassifier', SVC())]) \n",
    "pl_mm_svc = Pipeline([('Min Max Scaler', MinMaxScaler()),\n",
    "                      ('SVClassifier', SVC())])\n",
    "\n",
    "# Create list of pipelines\n",
    "pipelines = [\n",
    "    pl_std_tree, pl_mm_tree, \n",
    "    pl_std_knn, pl_mm_knn,\n",
    "    pl_mm_mnb, pl_mm_bnb, pl_std_gnb, \n",
    "    # pl_std_svc, pl_mm_svc\n",
    "]\n",
    "\n",
    "# Pipeline dictionary for labeling\n",
    "pipe_dict = {\n",
    "    0: 'SS+Tree', \n",
    "    1: 'MM+Tree',\n",
    "    2: 'SS+KNN',\n",
    "    3: 'MM+KNN',\n",
    "    4: 'MM+MNB',\n",
    "    5: 'MM+BNB',\n",
    "    6: 'SS+GNB',\n",
    "    #7: 'SS+SVC',\n",
    "    #8: 'MM+SVC'\n",
    "}\n",
    "\n",
    "# Handle mixed-type columns with non-numeric values (e.g., 'V58')\n",
    "problematic_columns = ['diag_1', 'diag_2', 'diag_3']  # Update these with actual columns causing issues\n",
    "for col in problematic_columns:\n",
    "    x_train[col] = x_train[col].apply(lambda x: float(x) if str(x).replace('.', '', 1).isdigit() else -1)\n",
    "    x_test[col] = x_test[col].apply(lambda x: float(x) if str(x).replace('.', '', 1).isdigit() else -1)\n",
    "\n",
    "# Fitting the data and evaluating models\n",
    "best_f1_score = 0\n",
    "\n",
    "for i, pipeline in enumerate(pipelines):\n",
    "    pipeline.fit(x_train, y_train)  # Fit the pipeline\n",
    "    y_pred = pipeline.predict(x_test)  # Predict on the test set\n",
    "\n",
    "    # Calculate precision, recall, f1-score\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    f1_score = report['weighted avg']['f1-score']\n",
    "    \n",
    "    print(f\"\\n{pipe_dict[i]} -> Classification Report:\")\n",
    "    \n",
    "    # Header for table\n",
    "    print(f\"{'Class':<15}{'Precision':<12}{'Recall':<12}{'F1-Score':<12}{'Support':<12}\")\n",
    "    \n",
    "    # Class-level metrics\n",
    "    for label, metrics in report.items():\n",
    "        if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            precision = metrics['precision']\n",
    "            recall = metrics['recall']\n",
    "            f1 = metrics['f1-score']\n",
    "            support = metrics['support']\n",
    "            print(f\"{label:<15}{precision:<12.4f}{recall:<12.4f}{f1:<12.4f}{support:<12}\")\n",
    "\n",
    "    # Overall metrics (averages and accuracy)\n",
    "    print(f\"{'Weighted Avg':<15}{report['weighted avg']['precision']:<12.4f}\"\n",
    "          f\"{report['weighted avg']['recall']:<12.4f}\"\n",
    "          f\"{report['weighted avg']['f1-score']:<12.4f}{'-':<12}\")\n",
    "    print(f\"{'Macro Avg':<15}{report['macro avg']['precision']:<12.4f}\"\n",
    "          f\"{report['macro avg']['recall']:<12.4f}\"\n",
    "          f\"{report['macro avg']['f1-score']:<12.4f}{'-':<12}\")\n",
    "    print(f\"{'Accuracy':<15}{'-':<12}{'-':<12}\"\n",
    "          f\"{f1_score:<12.4f}{'-':<12}\")\n",
    "    \n",
    "    # Identify the best-performing pipeline based on F1 score\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        best_pipeline = pipeline\n",
    "        best_scaler = pipe_dict[i]\n",
    "\n",
    "print(f\"\\nThe best pipeline for the dataset is {best_scaler} with an F1 score of {best_f1_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
